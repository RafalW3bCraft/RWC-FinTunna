{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafalW3bCraft/RWC-FinTunna/blob/main/RWC_FinTunna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RWC-FinTunna\n",
        "\n"
      ],
      "metadata": {
        "id": "rKIFmmatKy4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System_Configuretion"
      ],
      "metadata": {
        "id": "ljWtezWzmDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ðŸš€ SYSTEM INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âŒ No GPU detected! Please enable GPU in Runtime -> Change runtime type\")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb logging initially"
      ],
      "metadata": {
        "id": "zXBOse4TK2-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes xformers datasets -q\n",
        "!pip install transformers diffusers opencv-python pillow matplotlib scikit-learn pandas numpy -q\n",
        "\n",
        "print(\"âœ… Installation completed!\")"
      ],
      "metadata": {
        "id": "04thMAC-K4xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForImageClassification, AutoImageProcessor,\n",
        "    AutoModelForSequenceClassification, AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig, pipeline, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset, load_dataset\n",
        "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
        "from torchvision.transforms.functional import pil_to_tensor\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from transformers import\n",
        "import torch\n",
        "\n",
        "# Unsloth for efficient training [5][42][99]\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# image generation cap\n",
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "from torch import nn # Import nn for reinitializing the classifier\n",
        "import os # Import os for directory creation\n",
        "\n",
        "print(\"ðŸ”§ Setting up project configuration...\")"
      ],
      "metadata": {
        "id": "XXksxE8JLAjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectConfig:\n",
        "\n",
        "    # DistilBERT (Text Classification)\n",
        "    DISTILBERT_MODEL = \"distilbert-base-uncased\"\n",
        "    DISTILBERT_MAX_SEQ_LENGTH = 128 # Adjusted for memory\n",
        "    DISTILBERT_OUTPUT = \"./distilbert-finetuned\"\n",
        "\n",
        "    # MobileNetV2 (Image Classification - Placeholder, will need image data later)\n",
        "    MOBILENET_MODEL = \"google/mobilenet_v2_1.0_224\"\n",
        "    IMAGE_SIZE = 224 # Image size for MobileNetV2\n",
        "    MOBILENET_OUTPUT = \"./mobilenetv2-finetuned\"\n",
        "\n",
        "    # STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    # ROBERTA_MODEL = \"roberta-base\"\n",
        "    # FLAN_T5_MODEL = \"google/flan-t5-small\"\n",
        "\n",
        "    # Training parameters\n",
        "    MAX_SEQ_LENGTH = 1024                             # Reduced for T4 memory\n",
        "    BATCH_SIZE = 8                                    # Conservative for T4\n",
        "    GRADIENT_ACCUMULATION_STEPS = 1                   # Simplified\n",
        "    LEARNING_RATE = 5e-5                              # Safe Default\n",
        "    WARMUP_STEPS = 100\n",
        "    MAX_STEPS = 500\n",
        "    NUM_EPOCHS = 3 # Added for clarity\n",
        "\n",
        "    # LoRA for fine-tuning\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.05\n",
        "\n",
        "    # Quantization\n",
        "    USE_4BIT = True\n",
        "    BNB_4BIT_COMPUTE_DTYPE = torch.bfloat16\n",
        "    BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "    BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    STARCODER_OUTPUT = f\"{OUTPUT_DIR}/starcoder2-3b-finetuned\"\n",
        "    ROBERTA_OUTPUT = f\"{OUTPUT_DIR}/roberta-finetuned\"\n",
        "    FLAN_T5_OUTPUT = f\"{OUTPUT_DIR}/flan-t5-finetuned\"\n",
        "\n",
        "# for dir_path in [ProjectConfig.OUTPUT_DIR, ProjectConfig.STARCODER_OUTPUT, ProjectConfig.ROBERTA_OUTPUT, ProjectConfig.FLAN_T5_OUTPUT]\n",
        "\n",
        "for dir_path in [ProjectConfig.OUTPUT_DIR, ProjectConfig.DISTILBERT_OUTPUT,\n",
        "                 ProjectConfig.MOBILENET_OUTPUT]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# print(f\"  1. Code Generation: {ProjectConfig.STARCODER_MODEL}\")\n",
        "# print(f\"  2. Text Classification: {ProjectConfig.ROBERTA_MODEL}\")\n",
        "# print(f\"  3. Text-to-Text: {ProjectConfig.FLAN_T5_MODEL}\")\n",
        "print(f\"  1. Text Classification: {ProjectConfig.DISTILBERT_MODEL}\")\n",
        "print(f\"  2. Image Classification: {ProjectConfig.MOBILENET_MODEL}\")"
      ],
      "metadata": {
        "id": "HC_mAhiXLt1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ðŸ”‹ GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
        "        print(f\"ðŸ”‹ GPU Memory - Reserved: {torch.cuda.memory_reserved()/1024**3:.1f}GB\")\n",
        "    else:\n",
        "        print(\"âŒ No GPU available\")\n",
        "\n",
        "def clear_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"ðŸ§¹ GPU memory cleared\")\n",
        "\n",
        "'''\n",
        "def setup_quantization_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=ProjectConfig.USE_4BIT,\n",
        "        bnb_4bit_compute_dtype=ProjectConfig.BNB_4BIT_COMPUTE_DTYPE,\n",
        "        bnb_4bit_quant_type=ProjectConfig.BNB_4BIT_QUANT_TYPE,\n",
        "        bnb_4bit_use_double_quant=ProjectConfig.BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "    )\n",
        "'''\n",
        "print(\"ðŸ› ï¸ Utility functions ready!\")\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "id": "K7n7f3z3MJ-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "try:\n",
        "    notebook_login()\n",
        "    print(\"âœ… Ready to proceed (authentication skipped)\")\n",
        "except Exception as e:\n",
        "    print(\"failed continuing without authentication...\")\n"
      ],
      "metadata": {
        "id": "9UeuSRAZMT4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def setup_starcoder2_model():\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=ProjectConfig.STARCODER_MODEL,\n",
        "        max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        dtype=None,  # Auto-detect\n",
        "        load_in_4bit=True,\n",
        "        # token=\"hf_...\" # Uncomment if using private models\n",
        "    )\n",
        "\n",
        "    # Add LoRA fine-tuning\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=ProjectConfig.LORA_R,\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        lora_alpha=ProjectConfig.LORA_ALPHA,\n",
        "        lora_dropout=ProjectConfig.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… StarCoder2-3B loaded successfully!\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "starcoder_model, starcoder_tokenizer = setup_starcoder2_model()\n",
        "print(\"ðŸ“ StarCoder2-3B setup function ready!\")\n",
        "'''"
      ],
      "metadata": {
        "id": "kIvdLncSMn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def setup_roberta_model(num_labels=2):\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "\n",
        "    quantization_config = setup_quantization_config()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ProjectConfig.ROBERTA_MODEL,\n",
        "        num_labels=num_labels,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=ProjectConfig.LORA_R,\n",
        "        lora_alpha=ProjectConfig.LORA_ALPHA,\n",
        "        target_modules=[\"query\", \"value\", \"key\", \"dense\"],\n",
        "        lora_dropout=ProjectConfig.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(f\"âœ… RoBERTa Base loaded successfully!\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "roberta_model, roberta_tokenizer = setup_roberta_model()\n",
        "print(\"ðŸ“ RoBERTa Base setup function ready!\")\n",
        "'''"
      ],
      "metadata": {
        "id": "YJ_k6AW5MobL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove LoRA and complex quantization to save memory.\n",
        "\n",
        "def setup_distilbert_model(num_labels=2):\n",
        "    clear_memory() # Clear memory before loading a new model\n",
        "    print(\"Loading DistilBERT model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.DISTILBERT_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ProjectConfig.DISTILBERT_MODEL,\n",
        "        num_labels=num_labels,\n",
        "        # No quantization or LoRA applied here to reduce memory footprint.\n",
        "    )\n",
        "    print(f\"âœ… DistilBERT Base loaded successfully with {num_labels} labels!\")\n",
        "    print_gpu_utilization() # Print GPU utilization after loading\n",
        "    return model, tokenizer\n",
        "\n",
        "distilbert_model, distilbert_tokenizer = setup_distilbert_model(num_labels=2)\n",
        "print(\"ðŸ“ New model setup functions ready and models loaded!\")"
      ],
      "metadata": {
        "id": "Io1QofRvAcOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_mobilenet_model(num_labels=10): # Assuming 10 labels for a dataset like CIFAR-10\n",
        "    clear_memory()\n",
        "    print(\"Loading MobileNetV2 model...\")\n",
        "    processor = AutoImageProcessor.from_pretrained(ProjectConfig.MOBILENET_MODEL)\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        ProjectConfig.MOBILENET_MODEL,\n",
        "        ignore_mismatched_sizes=True,\n",
        "        # No quantization or LoRA applied here for memory efficiency.\n",
        "    )\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_ftrs, num_labels)\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "mobilenet_model, mobilenet_processor = setup_mobilenet_model(num_labels=10)\n",
        "print(\"ðŸ“ New model setup functions ready and models loaded!\")"
      ],
      "metadata": {
        "id": "1M6fSf2FBBzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nðŸ’¾ Output directories created:\")\n",
        "for output_dir in [ProjectConfig.STARCODER_OUTPUT, ProjectConfig.ROBERTA_OUTPUT, ProjectConfig.FLAN_T5_OUTPUT]:\n",
        "    print(f\"  â€¢ {output_dir}\")\n",
        "\n",
        "print(\"\\nðŸ”„ Ready to begin Phase One implementation!\")\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "id": "ljoPIZfiNa-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1ï¸âƒ£ Code Generation / Programming Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes / Language | Link |\n",
        "|---|---------|------------|-----------------|------|\n",
        "| 1 | `codeparrot/codeparrot-clean` | Clean Python GitHub code | Python | [Link](https://huggingface.co/datasets/codeparrot/codeparrot-clean) |\n",
        "| 2 | `bigcode/the-stack` | Large multi-language code corpus | Multi-language | [Link](https://huggingface.co/datasets/bigcode/the-stack) |\n",
        "| 3 | `bigcode/the-stack-smol` | Small subset for testing | Multi-language | [Link](https://huggingface.co/datasets/bigcode/the-stack-smol) |\n",
        "| 4 | `codeparrot/github-jupyter-text` | Jupyter notebooks, code + markdown | Python | [Link](https://huggingface.co/datasets/codeparrot/github-jupyter-text) |\n",
        "| 5 | `codeparrot/cleaned-java` | Cleaned Java code | Java | [Link](https://huggingface.co/datasets/codeparrot/cleaned-java) |\n",
        "| 6 | `codeparrot/cleaned-cpp` | Cleaned C++ code | C++ | [Link](https://huggingface.co/datasets/codeparrot/cleaned-cpp) |\n",
        "| 7 | `codeparrot/cleaned-javascript` | Cleaned JS code | JavaScript | [Link](https://huggingface.co/datasets/codeparrot/cleaned-javascript) |\n",
        "| 8 | `HuggingFace Code Dataset hub` | Collection of code datasets | Multiple | [Link](https://huggingface.co/datasets?search=code) |\n",
        "| 9 | `CodeSearchNet` | Code + docstring pairs for search | Python, Java, Go, etc. | [Link](https://huggingface.co/datasets/code_search_net) |\n",
        "| 10 | `CodeContests` | Competitive programming solutions | Python, C++ | [Link](https://huggingface.co/datasets/codecontests) |\n",
        "| 11 | `HackerRank` | Code challenges and solutions | Python, Java, C++ | [Link](https://www.kaggle.com/datasets/anishathalye/hackerrank-solutions) |\n",
        "| 12 | `Funcom` | Code summarization dataset | Python | [Link](https://huggingface.co/datasets/funcom) |\n",
        "| 13 | `CoNaLa` | Code/Natural Language dataset | Python | [Link](https://huggingface.co/datasets/conala) |\n",
        "| 14 | `Java-Large` | Large Java dataset | Java | [Link](https://huggingface.co/datasets/mbpp) |\n",
        "| 15 | `CodeXGLUE` | Multiple code tasks (translation, repair, summarization) | Multi-language | [Link](https://github.com/microsoft/CodeXGLUE) |\n",
        "| 16 | `MBPP` | Python code + natural language | Python | [Link](https://huggingface.co/datasets/mbpp) |\n",
        "| 17 | `CodeNet` | Large-scale multi-language code dataset | Multi-language | [Link](https://huggingface.co/datasets/codexglue) |\n",
        "\n",
        "---\n",
        "\n",
        "# **2ï¸âƒ£ NLP / Text Classification / Sentiment Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes | Link |\n",
        "|---|---------|------------|------|------|\n",
        "| 1 | `glue/sst2` | Stanford Sentiment Treebank | Binary sentiment | [Link](https://huggingface.co/datasets/glue/viewer/sst2) |\n",
        "| 2 | IMDB | Movie reviews, binary sentiment | English | [Link](https://huggingface.co/datasets/imdb) |\n",
        "| 3 | Amazon Reviews | Multi-domain reviews | Multi-class ratings | [Link](https://huggingface.co/datasets/amazon_reviews_multi) |\n",
        "| 4 | Yelp Reviews | User reviews, multi-class | English | [Link](https://huggingface.co/datasets/yelp_review_full) |\n",
        "| 5 | Emotion | Multi-class emotion labels | anger, joy, sadness, etc. | [Link](https://huggingface.co/datasets/emotion) |\n",
        "| 6 | TweetEval | Twitter sentiment | Short informal texts | [Link](https://huggingface.co/datasets/tweeteval) |\n",
        "| 7 | AG News | News classification | 4 classes | [Link](https://huggingface.co/datasets/ag_news) |\n",
        "| 8 | DBpedia | Wikipedia text classification | 14 classes | [Link](https://huggingface.co/datasets/dbpedia_14) |\n",
        "| 9 | 20 Newsgroups | Topic classification | 20 classes | [Link](https://huggingface.co/datasets/20_newsgroups) |\n",
        "| 10 | CoLA | Grammatical acceptability | Binary | [Link](https://huggingface.co/datasets/glue/viewer/cola) |\n",
        "| 11 | TREC | Question classification | 6 classes | [Link](https://huggingface.co/datasets/trec) |\n",
        "| 12 | Financial PhraseBank | Financial sentiment | Positive, negative, neutral | [Link](https://huggingface.co/datasets/financial_phrasebank) |\n",
        "| 13 | Toxic Comment Classification | Multi-label toxicity | English | [Link](https://huggingface.co/datasets/jigsaw_toxicity_pred) |\n",
        "| 14 | Multi-Domain Sentiment Dataset (MDSD) | Multi-domain reviews | English | [Link](https://huggingface.co/datasets/mdsd) |\n",
        "| 15 | Yelp Polarity | Binary sentiment | English | [Link](https://huggingface.co/datasets/yelp_polarity) |\n",
        "| 16 | Amazon Polarity | Binary sentiment | English | [Link](https://huggingface.co/datasets/amazon_polarity) |\n",
        "| 17 | SMS Spam Collection | Spam detection | English | [Link](https://huggingface.co/datasets/sms_spam) |\n",
        "\n",
        "---\n",
        "\n",
        "# **3ï¸âƒ£ Instruction-Tuning / Instruction-Following Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes | Link |\n",
        "|---|---------|------------|------|------|\n",
        "| 1 | `yahma/alpaca-cleaned` | Instruction + output | 52k examples | [Link](https://huggingface.co/datasets/yahma/alpaca-cleaned) |\n",
        "| 2 | Stanford Self-Instruct | Multi-domain instructions | 175k+ | [Link](https://huggingface.co/datasets/StanfordAI/self_instruct) |\n",
        "| 3 | OpenAssistant/oasst1 | Open-source ChatGPT-like instructions | 160k+ | [Link](https://huggingface.co/datasets/OpenAssistant/oasst1) |\n",
        "| 4 | ShareGPT | Conversations scraped from ChatGPT | Multi-turn dialogue | [Link](https://huggingface.co/datasets/ShareGPT) |\n",
        "| 5 | Dolly 2.0 | Instruction + output | 15k examples | [Link](https://huggingface.co/datasets/databricks/dolly_2_0) |\n",
        "| 6 | WizardLM | Instruction + reasoning dataset | 100k+ | [Link](https://huggingface.co/datasets/anon8231489123/wizardlm_unfiltered) |\n",
        "| 7 | Alpaca-GPT4 | GPT-4 generated instructions | 52k examples | [Link](https://huggingface.co/datasets/tatsu-lab/alpaca_gpt4) |\n",
        "| 8 | Evol-Instruct | Evolutionary instruction dataset | 100k+ | [Link](https://huggingface.co/datasets/evol-instruct) |\n",
        "| 9 | FLAN v2 | Googleâ€™s FLAN instruction dataset | Multi-task | [Link](https://huggingface.co/datasets/google/flan_v2) |\n",
        "| 10 | Tulu | Multilingual instruction dataset | 80k+ | [Link](https://huggingface.co/datasets/tulu) |\n",
        "| 11 | CoT-Instruct | Chain-of-thought instructions | Reasoning tasks | [Link](https://huggingface.co/datasets/cot_instruct) |\n",
        "| 12 | Super-NaturalInstructions | 1,600+ diverse tasks | Multi-task | [Link](https://huggingface.co/datasets/super_natural_instructions) |\n",
        "| 13 | MPT-Instruct | MosaicML instruction dataset | Multi-domain | [Link](https://huggingface.co/datasets/mpt_instruct) |\n",
        "| 14 | Vicuna Dataset | 70k+ instruction-response pairs | Open-source ChatGPT-like | [Link](https://huggingface.co/datasets/vicuna_dataset) |\n",
        "| 15 | OpenInstruct | 100k+ instructions | Multi-domain | [Link](https://huggingface.co/datasets/openinstruct) |\n",
        "| 16 | LIMA | Fine-tuning dataset for instruction-following | ~1k high-quality examples | [Link](https://github.com/StanfordAI/LIMA) |\n",
        "| 17 | Koala | Instruction dataset | ~100k examples | [Link](https://github.com/karpathy/koala) |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GmlB4f4QmQPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "QwZ3d_amZANG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    MAX_SEQ_LENGTH = 1024   # adjust per your VRAM (e.g., 512, 2048)\n",
        "    DATASET_SPLIT = \"train[:1%]\"  # smaller split for testing\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "code_dataset = load_dataset(\"codeparrot/codeparrot-clean\", split=ProjectConfig.DATASET_SPLIT)\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.STARCODER_MODEL)\n",
        "\n",
        "def preprocess_code(example):\n",
        "    code_sample = example[\"content\"]\n",
        "\n",
        "    tokenized = starcoder_tokenizer(\n",
        "        code_sample,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    )\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_code_dataset = code_dataset.map(\n",
        "    preprocess_code,\n",
        "    remove_columns=code_dataset.column_names,\n",
        "    batched=True,\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "print(\"âœ… StarCoder2-3B dataset loaded & tokenized!\")\n",
        "print(tokenized_code_dataset)\n"
      ],
      "metadata": {
        "id": "YiYBv9ugUnSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    ROBERTA_MODEL = \"roberta-base\"\n",
        "    MAX_SEQ_LENGTH = 128\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "\n",
        "print(\"Loading GLUE SST-2 dataset...\")\n",
        "glue_train = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "glue_val = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "def preprocess_glue(example):\n",
        "    return roberta_tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = glue_train.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_train.column_names,\n",
        "    desc=\"Tokenizing train split\"\n",
        ")\n",
        "\n",
        "tokenized_val = glue_val.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_val.column_names,\n",
        "    desc=\"Tokenizing validation split\"\n",
        ")\n",
        "\n",
        "print(\"âœ… RoBERTa Base dataset loaded & tokenized!\")\n",
        "print(tokenized_train)\n"
      ],
      "metadata": {
        "id": "W6rejVvgWEnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    FLAN_T5_MODEL = \"google/flan-t5-base\"\n",
        "    MAX_SEQ_LENGTH = 256\n",
        "    DATASET_SPLIT = \"train[:5%]\"  # small slice for testing\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "flan_t5_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.FLAN_T5_MODEL)\n",
        "\n",
        "print(\"Loading Alpaca-cleaned dataset...\")\n",
        "flan_t5_data = load_dataset(\"yahma/alpaca-cleaned\", split=ProjectConfig.DATASET_SPLIT)\n",
        "\n",
        "def preprocess_alpaca(example):\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    prompt = example[\"instruction\"]\n",
        "    if input_text.strip():\n",
        "        prompt += \"\\n\" + input_text\n",
        "\n",
        "    target = example[\"output\"]\n",
        "\n",
        "    model_inputs = flan_t5_tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    labels = flan_t5_tokenizer(\n",
        "        target,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing Alpaca dataset...\")\n",
        "tokenized_alpaca = flan_t5_data.map(\n",
        "    preprocess_alpaca,\n",
        "    batched=False,\n",
        "    remove_columns=flan_t5_data.column_names,\n",
        "    desc=\"Tokenizing Alpaca\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Flan-T5 dataset loaded & tokenized!\")\n",
        "print(tokenized_alpaca[0])\n"
      ],
      "metadata": {
        "id": "o2OPLaR-WJJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Loading GLUE SST-2 (DistilBERT) ===\")\n",
        "distilbert_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.DISTILBERT_MODEL)\n",
        "\n",
        "try:\n",
        "    # Load the dataset\n",
        "    glue_train = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "    glue_val = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "    print(\"GLUE SST-2 dataset loaded.\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading GLUE SST-2 dataset:\", e)\n",
        "\n",
        "def preprocess_glue(example):\n",
        "    result = distilbert_tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.DISTILBERT_MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    result[\"labels\"] = example[\"label\"]\n",
        "    return result\n",
        "\n",
        "print(\"Tokenizing GLUE SST-2 datasets...\")\n",
        "tokenized_train = glue_train.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_train.column_names, # Remove COLIUMS\n",
        "    desc=\"Tokenizing GLUE train split\"\n",
        ")\n",
        "tokenized_val = glue_val.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_val.column_names,\n",
        "    desc=\"Tokenizing GLUE validation split\"\n",
        ")\n",
        "print(\"\\nSample from tokenized GLUE train (DistilBERT):\")\n",
        "print(tokenized_train[0])"
      ],
      "metadata": {
        "id": "OHQEztyLB2OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Loading CIFAR-10 (MobileNetV2) ===\")\n",
        "mobilenet_processor = AutoImageProcessor.from_pretrained(ProjectConfig.MOBILENET_MODEL)\n",
        "\n",
        "try:\n",
        "    # Load the image dataset\n",
        "    cifar10_train = load_dataset(\"cifar10\", split=\"train\")\n",
        "    cifar10_val = load_dataset(\"cifar10\", split=\"test\") # CIFAR-10 uses 'test' for validation/evaluation\n",
        "    print(\"CIFAR-10 dataset loaded.\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading CIFAR-10 dataset:\", e)\n",
        "\n",
        "normalize = Normalize(mean=mobilenet_processor.image_mean, std=mobilenet_processor.image_std)\n",
        "# Compose transformations: resize, convert to tensor, normalize.\n",
        "_transforms = Compose([\n",
        "    RandomResizedCrop(ProjectConfig.IMAGE_SIZE),\n",
        "    ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "def preprocess_image(example):\n",
        "    example[\"pixel_values\"] = [_transforms(image.convert(\"RGB\")) for image in example[\"img\"]]\n",
        "    # Include the original label for training.\n",
        "    example[\"labels\"] = example[\"label\"]\n",
        "    return example\n",
        "\n",
        "\n",
        "\n",
        "print(\"Processing CIFAR-10 datasets...\")\n",
        "processed_cifar10_train = cifar10_train.map(\n",
        "    preprocess_image,\n",
        "    batched=True,\n",
        "    remove_columns=cifar10_train.column_names,\n",
        "    desc=\"Processing CIFAR-10 train split\"\n",
        ")\n",
        "\n",
        "\n",
        "processed_cifar10_val = cifar10_val.map(\n",
        "    preprocess_image,\n",
        "    batched=True,\n",
        "    remove_columns=cifar10_val.column_names,\n",
        "    desc=\"Processing CIFAR-10 validation split\"\n",
        ")\n",
        "print(\"âœ… CIFAR-10 dataset loaded & processed for MobileNetV2!\")\n",
        "\n",
        "print(\"\\nSample from processed CIFAR-10 train (MobileNetV2):\")\n",
        "print(processed_cifar10_train[0])\n"
      ],
      "metadata": {
        "id": "HVqCJ96MCqze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Arguments"
      ],
      "metadata": {
        "id": "_PzyeXRYEQE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_starcoder_training_args():\n",
        "    return SFTConfig(\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        max_steps=ProjectConfig.MAX_STEPS,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=ProjectConfig.STARCODER_OUTPUT,\n",
        "        max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    )\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "6UPeI6Y4Eafa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_roberta_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.ROBERTA_OUTPUT,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        dataloader_pin_memory=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "'''"
      ],
      "metadata": {
        "id": "N8Gvyj0ZEmuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_flan_t5_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.FLAN_T5_OUTPUT,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        predict_with_generate=True,\n",
        "        dataloader_pin_memory=False,\n",
        "    )\n",
        "'''"
      ],
      "metadata": {
        "id": "7CkndoTtEm6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distilbert_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.DISTILBERT_OUTPUT,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        num_train_epochs=ProjectConfig.NUM_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        logging_dir=\"./logs\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 7,\n",
        "        report_to=\"none\",\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "    )"
      ],
      "metadata": {
        "id": "MfKj3eIiERKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mobilenet_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.MOBILENET_OUTPUT,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        num_train_epochs=ProjectConfig.NUM_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        logging_dir=\"./logs\", # Directory for logs\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 7,\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "    )"
      ],
      "metadata": {
        "id": "Ej8nZIQEFLGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loops"
      ],
      "metadata": {
        "id": "2WOEaHh2lSFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_trainer = Trainer(\n",
        "    model=distilbert_model, # Loaded DistilBERT model\n",
        "    args=get_distilbert_training_args(), # DistilBERT training arguments\n",
        "    train_dataset=tokenized_train, # Tokenized GLUE train dataset\n",
        "    eval_dataset=tokenized_val, # Tokenized GLUE validation dataset\n",
        "    tokenizer=distilbert_tokenizer, # Tokenizer for padding\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=distilbert_tokenizer)\n",
        ")\n",
        "\n",
        "print(\"DistilBERT fine-tuning in progress...\")\n",
        "try:\n",
        "    distilbert_trainer.train()\n",
        "    print(\"DistilBERT training completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during DistilBERT training: {e}\")\n",
        "\n",
        "print(f\"Saving fine-tuned DistilBERT model to {ProjectConfig.DISTILBERT_OUTPUT} ...\")\n",
        "os.makedirs(ProjectConfig.DISTILBERT_OUTPUT, exist_ok=True)\n",
        "distilbert_model.save_pretrained(ProjectConfig.DISTILBERT_OUTPUT)\n",
        "distilbert_tokenizer.save_pretrained(ProjectConfig.DISTILBERT_OUTPUT)\n",
        "print(\"âœ… DistilBERT model saved!\")"
      ],
      "metadata": {
        "id": "AxAPnzfHF72Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_trainer = Trainer(\n",
        "    model=mobilenet_model, # Loaded MobileNetV2 model\n",
        "    args=get_mobilenet_training_args(), # MobileNetV2 training arguments\n",
        "    train_dataset=processed_cifar10_train, # Processed CIFAR-10 train dataset\n",
        "    eval_dataset=processed_cifar10_val, # Processed CIFAR-10 validation dataset\n",
        ")\n",
        "\n",
        "print(\"MobileNetV2 fine-tuning in progress...\")\n",
        "try:\n",
        "    mobilenet_trainer.train()\n",
        "    print(\"MobileNetV2 training completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during MobileNetV2 training: {e}\")\n",
        "\n",
        "print(f\"Saving fine-tuned MobileNetV2 model to {ProjectConfig.MOBILENET_OUTPUT} ...\") # Image models typically use a processor instead of a tokenizer.\n",
        "os.makedirs(ProjectConfig.MOBILENET_OUTPUT, exist_ok=True)\n",
        "mobilenet_model.save_pretrained(ProjectConfig.MOBILENET_OUTPUT)\n",
        "mobilenet_processor.save_pretrained(ProjectConfig.MOBILENET_OUTPUT)\n",
        "print(\"âœ… MobileNetV2 model saved!\")\n",
        "print(\"\\nTraining loops for MobileNetV2 completed.\")"
      ],
      "metadata": {
        "id": "CHYbfgcBGgO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "starcoder_trainer = SFTTrainer(\n",
        "    model=starcoder_model,\n",
        "    tokenizer=starcoder_tokenizer,\n",
        "    train_dataset=tokenized_code_dataset,\n",
        "    eval_dataset=None,  # add a validation split for real tasks\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"input_ids\",\n",
        "    max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "print(\"Starting StarCoder2-3B fine-tuning...\")\n",
        "starcoder_trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "print(f\"Saving fine-tuned model to {ProjectConfig.STARCODER_OUTPUT} ...\")\n",
        "starcoder_model.save_pretrained(ProjectConfig.STARCODER_OUTPUT)\n",
        "starcoder_tokenizer.save_pretrained(ProjectConfig.STARCODER_OUTPUT)\n",
        "print(\"âœ… Model saved!\")\n",
        "print(\"\\nTraining loops for starcoder completed.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "_4atv726Y3rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "roberta_trainer = Trainer(\n",
        "    model=roberta_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=roberta_tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Starting RoBERTa SST-2 fine-tuning...\")\n",
        "roberta_trainer.train()\n",
        "\n",
        "print(f\"Saving fine-tuned model to {ProjectConfig.ROBERTA_OUTPUT} ...\")\n",
        "roberta_model.save_pretrained(ProjectConfig.ROBERTA_OUTPUT)\n",
        "roberta_tokenizer.save_pretrained(ProjectConfig.ROBERTA_OUTPUT)\n",
        "print(\"âœ… RoBERTa SST-2 model saved!\")\n",
        "print(\"\\nTraining loops for RoBERTa SST-2 completed.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "oeUi3jzpZ2R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "flan_trainer = Trainer(\n",
        "    model=flan_t5_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_alpaca,\n",
        "    eval_dataset=None,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=flan_t5_tokenizer\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting Flan-T5 Alpaca fine-tuning...\")\n",
        "flan_trainer.train()\n",
        "\n",
        "print(f\"Saving fine-tuned Flan-T5 model to {ProjectConfig.FLAN_T5_OUTPUT} ...\")\n",
        "flan_t5_model.save_pretrained(ProjectConfig.FLAN_T5_OUTPUT)\n",
        "flan_t5_tokenizer.save_pretrained(ProjectConfig.FLAN_T5_OUTPUT)\n",
        "print(\"âœ… Flan-T5 model saved!\")\n",
        "print(\"\\nTraining loops for Flan-T5 completed.\")\n",
        "'''"
      ],
      "metadata": {
        "id": "65sqUUqKZ2Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1e7357a"
      },
      "source": [
        "# Task\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgT2waHwFRgw"
      },
      "source": [
        "\n",
        "print_gpu_utilization()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}