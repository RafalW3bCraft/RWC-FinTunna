{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM43dvoI0xGI1+KuMpiUNUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafalW3bCraft/RWC-FinTunna/blob/main/RWC_FinTunna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RWC-FinTunna\n",
        "\n"
      ],
      "metadata": {
        "id": "rKIFmmatKy4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System_Configuretion"
      ],
      "metadata": {
        "id": "ljWtezWzmDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ðŸš€ SYSTEM INFORMATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âŒ No GPU detected! Please enable GPU in Runtime -> Change runtime type\")\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb logging initially\n"
      ],
      "metadata": {
        "id": "zXBOse4TK2-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes xformers datasets -q\n",
        "!pip install transformers diffusers opencv-python pillow matplotlib scikit-learn pandas numpy -q\n",
        "\n",
        "print(\"âœ… Installation completed!\")"
      ],
      "metadata": {
        "id": "04thMAC-K4xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification, AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig, pipeline\n",
        ")\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# Unsloth for efficient training [5][42][99]\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# image generation cap\n",
        "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "print(\"ðŸ”§ Setting up project configuration...\")"
      ],
      "metadata": {
        "id": "XXksxE8JLAjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectConfig:\n",
        "\n",
        "    STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    ROBERTA_MODEL = \"roberta-base\"\n",
        "    FLAN_T5_MODEL = \"google/flan-t5-small\"\n",
        "\n",
        "    # Training parameters\n",
        "    MAX_SEQ_LENGTH = 1024                             # Reduced for T4 memory\n",
        "    BATCH_SIZE = 1                                    # Conservative for T4\n",
        "    GRADIENT_ACCUMULATION_STEPS = 8                   # Simulate larger batch\n",
        "    LEARNING_RATE = 2e-4                              # Standard for LoRA\n",
        "    WARMUP_STEPS = 100\n",
        "    MAX_STEPS = 500                                   # Start with smaller steps\n",
        "\n",
        "    # LoRA for fine-tuning\n",
        "    LORA_R = 16\n",
        "    LORA_ALPHA = 32\n",
        "    LORA_DROPOUT = 0.05\n",
        "\n",
        "    # Quantization\n",
        "    USE_4BIT = True\n",
        "    BNB_4BIT_COMPUTE_DTYPE = torch.bfloat16\n",
        "    BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
        "    BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "\n",
        "    OUTPUT_DIR = \"./outputs\"\n",
        "    STARCODER_OUTPUT = f\"{OUTPUT_DIR}/starcoder2-3b-finetuned\"\n",
        "    ROBERTA_OUTPUT = f\"{OUTPUT_DIR}/roberta-finetuned\"\n",
        "    FLAN_T5_OUTPUT = f\"{OUTPUT_DIR}/flan-t5-finetuned\"\n",
        "\n",
        "# Create output dir\n",
        "import os\n",
        "for dir_path in [ProjectConfig.OUTPUT_DIR, ProjectConfig.STARCODER_OUTPUT,\n",
        "                 ProjectConfig.ROBERTA_OUTPUT, ProjectConfig.FLAN_T5_OUTPUT]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(f\"  1. Code Generation: {ProjectConfig.STARCODER_MODEL}\")\n",
        "print(f\"  2. Text Classification: {ProjectConfig.ROBERTA_MODEL}\")\n",
        "print(f\"  3. Text-to-Text: {ProjectConfig.FLAN_T5_MODEL}\")\n"
      ],
      "metadata": {
        "id": "HC_mAhiXLt1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"ðŸ”‹ GPU Memory - Allocated: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
        "        print(f\"ðŸ”‹ GPU Memory - Reserved: {torch.cuda.memory_reserved()/1024**3:.1f}GB\")\n",
        "    else:\n",
        "        print(\"âŒ No GPU available\")\n",
        "\n",
        "def clear_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"ðŸ§¹ GPU memory cleared\")\n",
        "\n",
        "def setup_quantization_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=ProjectConfig.USE_4BIT,\n",
        "        bnb_4bit_compute_dtype=ProjectConfig.BNB_4BIT_COMPUTE_DTYPE,\n",
        "        bnb_4bit_quant_type=ProjectConfig.BNB_4BIT_QUANT_TYPE,\n",
        "        bnb_4bit_use_double_quant=ProjectConfig.BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "    )\n",
        "\n",
        "print(\"ðŸ› ï¸ Utility functions ready!\")\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "id": "K7n7f3z3MJ-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "try:\n",
        "    notebook_login()\n",
        "    print(\"âœ… Ready to proceed (authentication skipped)\")\n",
        "except Exception as e:\n",
        "    print(\"failed continuing without authentication...\")\n"
      ],
      "metadata": {
        "id": "9UeuSRAZMT4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_starcoder2_model():\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=ProjectConfig.STARCODER_MODEL,\n",
        "        max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        dtype=None,  # Auto-detect\n",
        "        load_in_4bit=True,\n",
        "        # token=\"hf_...\" # Uncomment if using private models\n",
        "    )\n",
        "\n",
        "    # Add LoRA fine-tuning\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=ProjectConfig.LORA_R,\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        lora_alpha=ProjectConfig.LORA_ALPHA,\n",
        "        lora_dropout=ProjectConfig.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "        loftq_config=None,\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… StarCoder2-3B loaded successfully!\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "starcoder_model, starcoder_tokenizer = setup_starcoder2_model()\n",
        "print(\"ðŸ“ StarCoder2-3B setup function ready!\")\n"
      ],
      "metadata": {
        "id": "kIvdLncSMn4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_roberta_model(num_labels=2):\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "\n",
        "    quantization_config = setup_quantization_config()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ProjectConfig.ROBERTA_MODEL,\n",
        "        num_labels=num_labels,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=ProjectConfig.LORA_R,\n",
        "        lora_alpha=ProjectConfig.LORA_ALPHA,\n",
        "        target_modules=[\"query\", \"value\", \"key\", \"dense\"],\n",
        "        lora_dropout=ProjectConfig.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(f\"âœ… RoBERTa Base loaded successfully!\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "roberta_model, roberta_tokenizer = setup_roberta_model()\n",
        "print(\"ðŸ“ RoBERTa Base setup function ready!\")\n"
      ],
      "metadata": {
        "id": "YJ_k6AW5MobL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_flan_t5_model():\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.FLAN_T5_MODEL)\n",
        "\n",
        "    quantization_config = setup_quantization_config()\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ProjectConfig.FLAN_T5_MODEL,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        r=ProjectConfig.LORA_R,\n",
        "        lora_alpha=ProjectConfig.LORA_ALPHA,\n",
        "        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n",
        "        lora_dropout=ProjectConfig.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(f\"âœ… Flan-T5-Small loaded successfully!\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "flan_t5_model, flan_t5_tokenizer = setup_flan_t5_model()\n",
        "print(\"ðŸ“ Flan-T5-Small setup function ready!\")\n"
      ],
      "metadata": {
        "id": "O2n4f3lBNAVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_starcoder_training_args():\n",
        "    return SFTConfig(\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        max_steps=ProjectConfig.MAX_STEPS,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=ProjectConfig.STARCODER_OUTPUT,\n",
        "        max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    )\n",
        "\n",
        "def get_roberta_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.ROBERTA_OUTPUT,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        dataloader_pin_memory=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "def get_flan_t5_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.FLAN_T5_OUTPUT,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=ProjectConfig.GRADIENT_ACCUMULATION_STEPS,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        logging_steps=10,\n",
        "        warmup_steps=ProjectConfig.WARMUP_STEPS,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        predict_with_generate=True,\n",
        "        dataloader_pin_memory=False,\n",
        "    )\n",
        "\n",
        "print(\"ðŸ“Š Next steps: Load datasets and start training individual models\")"
      ],
      "metadata": {
        "id": "oqa8eccpNLOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nðŸ’¾ Output directories created:\")\n",
        "for output_dir in [ProjectConfig.STARCODER_OUTPUT, ProjectConfig.ROBERTA_OUTPUT, ProjectConfig.FLAN_T5_OUTPUT]:\n",
        "    print(f\"  â€¢ {output_dir}\")\n",
        "\n",
        "print(\"\\nðŸ”„ Ready to begin Phase One implementation!\")\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "id": "ljoPIZfiNa-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1ï¸âƒ£ Code Generation / Programming Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes / Language | Link |\n",
        "|---|---------|------------|-----------------|------|\n",
        "| 1 | `codeparrot/codeparrot-clean` | Clean Python GitHub code | Python | [Link](https://huggingface.co/datasets/codeparrot/codeparrot-clean) |\n",
        "| 2 | `bigcode/the-stack` | Large multi-language code corpus | Multi-language | [Link](https://huggingface.co/datasets/bigcode/the-stack) |\n",
        "| 3 | `bigcode/the-stack-smol` | Small subset for testing | Multi-language | [Link](https://huggingface.co/datasets/bigcode/the-stack-smol) |\n",
        "| 4 | `codeparrot/github-jupyter-text` | Jupyter notebooks, code + markdown | Python | [Link](https://huggingface.co/datasets/codeparrot/github-jupyter-text) |\n",
        "| 5 | `codeparrot/cleaned-java` | Cleaned Java code | Java | [Link](https://huggingface.co/datasets/codeparrot/cleaned-java) |\n",
        "| 6 | `codeparrot/cleaned-cpp` | Cleaned C++ code | C++ | [Link](https://huggingface.co/datasets/codeparrot/cleaned-cpp) |\n",
        "| 7 | `codeparrot/cleaned-javascript` | Cleaned JS code | JavaScript | [Link](https://huggingface.co/datasets/codeparrot/cleaned-javascript) |\n",
        "| 8 | `HuggingFace Code Dataset hub` | Collection of code datasets | Multiple | [Link](https://huggingface.co/datasets?search=code) |\n",
        "| 9 | `CodeSearchNet` | Code + docstring pairs for search | Python, Java, Go, etc. | [Link](https://huggingface.co/datasets/code_search_net) |\n",
        "| 10 | `CodeContests` | Competitive programming solutions | Python, C++ | [Link](https://huggingface.co/datasets/codecontests) |\n",
        "| 11 | `HackerRank` | Code challenges and solutions | Python, Java, C++ | [Link](https://www.kaggle.com/datasets/anishathalye/hackerrank-solutions) |\n",
        "| 12 | `Funcom` | Code summarization dataset | Python | [Link](https://huggingface.co/datasets/funcom) |\n",
        "| 13 | `CoNaLa` | Code/Natural Language dataset | Python | [Link](https://huggingface.co/datasets/conala) |\n",
        "| 14 | `Java-Large` | Large Java dataset | Java | [Link](https://huggingface.co/datasets/mbpp) |\n",
        "| 15 | `CodeXGLUE` | Multiple code tasks (translation, repair, summarization) | Multi-language | [Link](https://github.com/microsoft/CodeXGLUE) |\n",
        "| 16 | `MBPP` | Python code + natural language | Python | [Link](https://huggingface.co/datasets/mbpp) |\n",
        "| 17 | `CodeNet` | Large-scale multi-language code dataset | Multi-language | [Link](https://huggingface.co/datasets/codexglue) |\n",
        "\n",
        "---\n",
        "\n",
        "# **2ï¸âƒ£ NLP / Text Classification / Sentiment Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes | Link |\n",
        "|---|---------|------------|------|------|\n",
        "| 1 | `glue/sst2` | Stanford Sentiment Treebank | Binary sentiment | [Link](https://huggingface.co/datasets/glue/viewer/sst2) |\n",
        "| 2 | IMDB | Movie reviews, binary sentiment | English | [Link](https://huggingface.co/datasets/imdb) |\n",
        "| 3 | Amazon Reviews | Multi-domain reviews | Multi-class ratings | [Link](https://huggingface.co/datasets/amazon_reviews_multi) |\n",
        "| 4 | Yelp Reviews | User reviews, multi-class | English | [Link](https://huggingface.co/datasets/yelp_review_full) |\n",
        "| 5 | Emotion | Multi-class emotion labels | anger, joy, sadness, etc. | [Link](https://huggingface.co/datasets/emotion) |\n",
        "| 6 | TweetEval | Twitter sentiment | Short informal texts | [Link](https://huggingface.co/datasets/tweeteval) |\n",
        "| 7 | AG News | News classification | 4 classes | [Link](https://huggingface.co/datasets/ag_news) |\n",
        "| 8 | DBpedia | Wikipedia text classification | 14 classes | [Link](https://huggingface.co/datasets/dbpedia_14) |\n",
        "| 9 | 20 Newsgroups | Topic classification | 20 classes | [Link](https://huggingface.co/datasets/20_newsgroups) |\n",
        "| 10 | CoLA | Grammatical acceptability | Binary | [Link](https://huggingface.co/datasets/glue/viewer/cola) |\n",
        "| 11 | TREC | Question classification | 6 classes | [Link](https://huggingface.co/datasets/trec) |\n",
        "| 12 | Financial PhraseBank | Financial sentiment | Positive, negative, neutral | [Link](https://huggingface.co/datasets/financial_phrasebank) |\n",
        "| 13 | Toxic Comment Classification | Multi-label toxicity | English | [Link](https://huggingface.co/datasets/jigsaw_toxicity_pred) |\n",
        "| 14 | Multi-Domain Sentiment Dataset (MDSD) | Multi-domain reviews | English | [Link](https://huggingface.co/datasets/mdsd) |\n",
        "| 15 | Yelp Polarity | Binary sentiment | English | [Link](https://huggingface.co/datasets/yelp_polarity) |\n",
        "| 16 | Amazon Polarity | Binary sentiment | English | [Link](https://huggingface.co/datasets/amazon_polarity) |\n",
        "| 17 | SMS Spam Collection | Spam detection | English | [Link](https://huggingface.co/datasets/sms_spam) |\n",
        "\n",
        "---\n",
        "\n",
        "# **3ï¸âƒ£ Instruction-Tuning / Instruction-Following Datasets (17+)**\n",
        "\n",
        "| # | Dataset | Description | Notes | Link |\n",
        "|---|---------|------------|------|------|\n",
        "| 1 | `yahma/alpaca-cleaned` | Instruction + output | 52k examples | [Link](https://huggingface.co/datasets/yahma/alpaca-cleaned) |\n",
        "| 2 | Stanford Self-Instruct | Multi-domain instructions | 175k+ | [Link](https://huggingface.co/datasets/StanfordAI/self_instruct) |\n",
        "| 3 | OpenAssistant/oasst1 | Open-source ChatGPT-like instructions | 160k+ | [Link](https://huggingface.co/datasets/OpenAssistant/oasst1) |\n",
        "| 4 | ShareGPT | Conversations scraped from ChatGPT | Multi-turn dialogue | [Link](https://huggingface.co/datasets/ShareGPT) |\n",
        "| 5 | Dolly 2.0 | Instruction + output | 15k examples | [Link](https://huggingface.co/datasets/databricks/dolly_2_0) |\n",
        "| 6 | WizardLM | Instruction + reasoning dataset | 100k+ | [Link](https://huggingface.co/datasets/anon8231489123/wizardlm_unfiltered) |\n",
        "| 7 | Alpaca-GPT4 | GPT-4 generated instructions | 52k examples | [Link](https://huggingface.co/datasets/tatsu-lab/alpaca_gpt4) |\n",
        "| 8 | Evol-Instruct | Evolutionary instruction dataset | 100k+ | [Link](https://huggingface.co/datasets/evol-instruct) |\n",
        "| 9 | FLAN v2 | Googleâ€™s FLAN instruction dataset | Multi-task | [Link](https://huggingface.co/datasets/google/flan_v2) |\n",
        "| 10 | Tulu | Multilingual instruction dataset | 80k+ | [Link](https://huggingface.co/datasets/tulu) |\n",
        "| 11 | CoT-Instruct | Chain-of-thought instructions | Reasoning tasks | [Link](https://huggingface.co/datasets/cot_instruct) |\n",
        "| 12 | Super-NaturalInstructions | 1,600+ diverse tasks | Multi-task | [Link](https://huggingface.co/datasets/super_natural_instructions) |\n",
        "| 13 | MPT-Instruct | MosaicML instruction dataset | Multi-domain | [Link](https://huggingface.co/datasets/mpt_instruct) |\n",
        "| 14 | Vicuna Dataset | 70k+ instruction-response pairs | Open-source ChatGPT-like | [Link](https://huggingface.co/datasets/vicuna_dataset) |\n",
        "| 15 | OpenInstruct | 100k+ instructions | Multi-domain | [Link](https://huggingface.co/datasets/openinstruct) |\n",
        "| 16 | LIMA | Fine-tuning dataset for instruction-following | ~1k high-quality examples | [Link](https://github.com/StanfordAI/LIMA) |\n",
        "| 17 | Koala | Instruction dataset | ~100k examples | [Link](https://github.com/karpathy/koala) |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GmlB4f4QmQPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing"
      ],
      "metadata": {
        "id": "QwZ3d_amZANG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    MAX_SEQ_LENGTH = 1024   # adjust per your VRAM (e.g., 512, 2048)\n",
        "    DATASET_SPLIT = \"train[:1%]\"  # smaller split for testing\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "code_dataset = load_dataset(\"codeparrot/codeparrot-clean\", split=ProjectConfig.DATASET_SPLIT)\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.STARCODER_MODEL)\n",
        "\n",
        "def preprocess_code(example):\n",
        "    code_sample = example[\"content\"]\n",
        "\n",
        "    tokenized = starcoder_tokenizer(\n",
        "        code_sample,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    )\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_code_dataset = code_dataset.map(\n",
        "    preprocess_code,\n",
        "    remove_columns=code_dataset.column_names,\n",
        "    batched=True,\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "print(\"âœ… StarCoder2-3B dataset loaded & tokenized!\")\n",
        "print(tokenized_code_dataset)\n"
      ],
      "metadata": {
        "id": "YiYBv9ugUnSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    ROBERTA_MODEL = \"roberta-base\"\n",
        "    MAX_SEQ_LENGTH = 128\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "\n",
        "print(\"Loading GLUE SST-2 dataset...\")\n",
        "glue_train = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "glue_val = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "def preprocess_glue(example):\n",
        "    return roberta_tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_train = glue_train.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_train.column_names,\n",
        "    desc=\"Tokenizing train split\"\n",
        ")\n",
        "\n",
        "tokenized_val = glue_val.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_val.column_names,\n",
        "    desc=\"Tokenizing validation split\"\n",
        ")\n",
        "\n",
        "print(\"âœ… RoBERTa Base dataset loaded & tokenized!\")\n",
        "print(tokenized_train)\n"
      ],
      "metadata": {
        "id": "W6rejVvgWEnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    FLAN_T5_MODEL = \"google/flan-t5-base\"\n",
        "    MAX_SEQ_LENGTH = 256\n",
        "    DATASET_SPLIT = \"train[:5%]\"  # small slice for testing\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "flan_t5_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.FLAN_T5_MODEL)\n",
        "\n",
        "print(\"Loading Alpaca-cleaned dataset...\")\n",
        "flan_t5_data = load_dataset(\"yahma/alpaca-cleaned\", split=ProjectConfig.DATASET_SPLIT)\n",
        "\n",
        "def preprocess_alpaca(example):\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    prompt = example[\"instruction\"]\n",
        "    if input_text.strip():\n",
        "        prompt += \"\\n\" + input_text\n",
        "\n",
        "    target = example[\"output\"]\n",
        "\n",
        "    model_inputs = flan_t5_tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    labels = flan_t5_tokenizer(\n",
        "        target,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing Alpaca dataset...\")\n",
        "tokenized_alpaca = flan_t5_data.map(\n",
        "    preprocess_alpaca,\n",
        "    batched=False,\n",
        "    remove_columns=flan_t5_data.column_names,\n",
        "    desc=\"Tokenizing Alpaca\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Flan-T5 dataset loaded & tokenized!\")\n",
        "print(tokenized_alpaca[0])\n"
      ],
      "metadata": {
        "id": "o2OPLaR-WJJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unified # Dataset Preprocessing\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ProjectConfig:\n",
        "    # StarCoder2-3B (code)\n",
        "    STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    CODE_MAX_SEQ_LENGTH = 1024\n",
        "    CODE_SPLIT = \"train[:1%]\"\n",
        "\n",
        "    # RoBERTa Base (GLUE SST-2)\n",
        "    ROBERTA_MODEL = \"roberta-base\"\n",
        "    ROBERTA_MAX_SEQ_LENGTH = 128\n",
        "\n",
        "    # Flan-T5 Base (Instruction Tuning)\n",
        "    FLAN_T5_MODEL = \"google/flan-t5-base\"\n",
        "    FLAN_MAX_SEQ_LENGTH = 256\n",
        "    FLAN_SPLIT = \"train[:5%]\"\n",
        "\n",
        "# 1. StarCoder2-3B / CodeParrot\n",
        "print(\"\\n=== Loading StarCoder2-3B (CodeParrot) ===\")\n",
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.STARCODER_MODEL)\n",
        "try:\n",
        "    code_dataset = load_dataset(\"codeparrot/codeparrot-clean\", split=ProjectConfig.CODE_SPLIT)\n",
        "except Exception as e:\n",
        "    print(\"Error loading CodeParrot dataset:\", e)\n",
        "    exit(1)\n",
        "\n",
        "def preprocess_code(example):\n",
        "    tokenized = starcoder_tokenizer(\n",
        "        example[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.CODE_MAX_SEQ_LENGTH,\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "tokenized_code_dataset = code_dataset.map(\n",
        "    preprocess_code,\n",
        "    batched=True,\n",
        "    remove_columns=code_dataset.column_names,\n",
        "    desc=\"Tokenizing code dataset\"\n",
        ")\n",
        "print(\"âœ… StarCoder2-3B dataset loaded & tokenized!\")\n",
        "\n",
        "\n",
        "# 2. GLUE SST-2 / RoBERTa\n",
        "print(\"\\n=== Loading GLUE SST-2 (RoBERTa) ===\")\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "\n",
        "glue_train = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "glue_val = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "def preprocess_glue(example):\n",
        "    result = roberta_tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.ROBERTA_MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    result[\"labels\"] = example[\"label\"]\n",
        "    return result\n",
        "\n",
        "tokenized_train = glue_train.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_train.column_names,\n",
        "    desc=\"Tokenizing GLUE train split\"\n",
        ")\n",
        "tokenized_val = glue_val.map(\n",
        "    preprocess_glue,\n",
        "    batched=True,\n",
        "    remove_columns=glue_val.column_names,\n",
        "    desc=\"Tokenizing GLUE validation split\"\n",
        ")\n",
        "print(\"âœ… GLUE SST-2 dataset loaded & tokenized!\")\n",
        "\n",
        "\n",
        "# 3. Alpaca-Cleaned / Flan-T5\n",
        "print(\"\\n=== Loading Alpaca-Cleaned (Flan-T5) ===\")\n",
        "flan_t5_tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.FLAN_T5_MODEL)\n",
        "flan_t5_data = load_dataset(\"yahma/alpaca-cleaned\", split=ProjectConfig.FLAN_SPLIT)\n",
        "\n",
        "def preprocess_alpaca(example):\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    prompt = example[\"instruction\"]\n",
        "    if input_text.strip():\n",
        "        prompt += \"\\n\" + input_text\n",
        "    target = example[\"output\"]\n",
        "\n",
        "    model_inputs = flan_t5_tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.FLAN_MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    labels = flan_t5_tokenizer(\n",
        "        target,\n",
        "        truncation=True,\n",
        "        max_length=ProjectConfig.FLAN_MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_alpaca = flan_t5_data.map(\n",
        "    preprocess_alpaca,\n",
        "    batched=False,\n",
        "    remove_columns=flan_t5_data.column_names,\n",
        "    desc=\"Tokenizing Alpaca dataset\"\n",
        ")\n",
        "print(\"âœ… Alpaca-cleaned dataset loaded & tokenized!\")\n",
        "\n",
        "\n",
        "print(\"Sample from tokenized code dataset:\", tokenized_code_dataset[0])\n",
        "print(\"Sample from tokenized GLUE train:\", tokenized_train[0])\n",
        "print(\"Sample from tokenized Alpaca:\", tokenized_alpaca[0])\n"
      ],
      "metadata": {
        "id": "VzcbdcdiXevk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is Training loops"
      ],
      "metadata": {
        "id": "2WOEaHh2lSFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "class ProjectConfig:\n",
        "    STARCODER_MODEL = \"bigcode/starcoder2-3b\"\n",
        "    STARCODER_OUTPUT = \"./starcoder2-finetuned\"\n",
        "    MAX_SEQ_LENGTH = 1024\n",
        "\n",
        "def setup_starcoder2_model():\n",
        "    print(\"Loading StarCoder2-3B model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        ProjectConfig.STARCODER_MODEL,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,  # adjust to float32 if GPU memory is tight\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.STARCODER_MODEL)\n",
        "    return model, tokenizer\n",
        "\n",
        "starcoder_model, starcoder_tokenizer = setup_starcoder2_model()\n",
        "\n",
        "\n",
        "# Apply LoRA (PEFT)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # StarCoder2-3B attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "starcoder_model = get_peft_model(starcoder_model, lora_config)\n",
        "\n",
        "# Define Training Args\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "def get_starcoder_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.STARCODER_OUTPUT,\n",
        "        per_device_train_batch_size=1,  # adjust to VRAM\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        max_steps=1000,  # small number for testing\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "training_args = get_starcoder_training_args()\n",
        "\n",
        "# Setup SFTTrainer\n",
        "starcoder_trainer = SFTTrainer(\n",
        "    model=starcoder_model,\n",
        "    tokenizer=starcoder_tokenizer,\n",
        "    train_dataset=tokenized_code_dataset,\n",
        "    eval_dataset=None,  # add a validation split for real tasks\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"input_ids\",\n",
        "    max_seq_length=ProjectConfig.MAX_SEQ_LENGTH,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "print(\"Starting StarCoder2-3B fine-tuning...\")\n",
        "starcoder_trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "print(f\"Saving fine-tuned model to {ProjectConfig.STARCODER_OUTPUT} ...\")\n",
        "starcoder_model.save_pretrained(ProjectConfig.STARCODER_OUTPUT)\n",
        "starcoder_tokenizer.save_pretrained(ProjectConfig.STARCODER_OUTPUT)\n",
        "print(\"âœ… Model saved!\")\n"
      ],
      "metadata": {
        "id": "_4atv726Y3rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "class ProjectConfig:\n",
        "    ROBERTA_MODEL = \"roberta-base\"\n",
        "    ROBERTA_MAX_SEQ_LENGTH = 128\n",
        "    ROBERTA_OUTPUT = \"./roberta-sst2-finetuned\"\n",
        "    TRAIN_BATCH_SIZE = 16\n",
        "    EVAL_BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 2e-5\n",
        "    NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "def setup_roberta_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.ROBERTA_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        ProjectConfig.ROBERTA_MODEL,\n",
        "        num_labels=2  # SST-2 is binary classification\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "roberta_model, roberta_tokenizer = setup_roberta_model()\n",
        "\n",
        "\n",
        "def get_roberta_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.ROBERTA_OUTPUT,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        per_device_train_batch_size=ProjectConfig.TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.EVAL_BATCH_SIZE,\n",
        "        num_train_epochs=ProjectConfig.NUM_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        logging_dir=\"./logs\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "training_args = get_roberta_training_args()\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
        "\n",
        "roberta_trainer = Trainer(\n",
        "    model=roberta_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=roberta_tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Starting RoBERTa SST-2 fine-tuning...\")\n",
        "roberta_trainer.train()\n",
        "\n",
        "print(f\"Saving fine-tuned model to {ProjectConfig.ROBERTA_OUTPUT} ...\")\n",
        "roberta_model.save_pretrained(ProjectConfig.ROBERTA_OUTPUT)\n",
        "roberta_tokenizer.save_pretrained(ProjectConfig.ROBERTA_OUTPUT)\n",
        "print(\"âœ… RoBERTa SST-2 model saved!\")\n"
      ],
      "metadata": {
        "id": "oeUi3jzpZ2R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "import torch\n",
        "\n",
        "class ProjectConfig:\n",
        "    FLAN_T5_MODEL = \"google/flan-t5-base\"\n",
        "    FLAN_MAX_SEQ_LENGTH = 256\n",
        "    FLAN_SPLIT = \"train[:5%]\"\n",
        "    FLAN_T5_OUTPUT = \"./flan-t5-alpaca-finetuned\"\n",
        "    TRAIN_BATCH_SIZE = 4\n",
        "    EVAL_BATCH_SIZE = 4\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 3\n",
        "\n",
        "def setup_flan_t5_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ProjectConfig.FLAN_T5_MODEL)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ProjectConfig.FLAN_T5_MODEL,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float32  # Changed to float32\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "flan_t5_model, flan_t5_tokenizer = setup_flan_t5_model()\n",
        "\n",
        "def get_flan_t5_training_args():\n",
        "    return TrainingArguments(\n",
        "        output_dir=ProjectConfig.FLAN_T5_OUTPUT,\n",
        "        per_device_train_batch_size=ProjectConfig.TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=ProjectConfig.EVAL_BATCH_SIZE,\n",
        "        num_train_epochs=ProjectConfig.NUM_EPOCHS,\n",
        "        learning_rate=ProjectConfig.LEARNING_RATE,\n",
        "        logging_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        fp16=False, # Disabled FP16\n",
        "        # evaluation_strategy=\"no\", # Removed as no eval dataset is provided\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "training_args = get_flan_t5_training_args()\n",
        "\n",
        "# Data collator for Seq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=flan_t5_tokenizer,\n",
        "    model=flan_t5_model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "flan_trainer = Trainer(\n",
        "    model=flan_t5_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_alpaca,\n",
        "    eval_dataset=None,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=flan_t5_tokenizer\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting Flan-T5 Alpaca fine-tuning...\")\n",
        "flan_trainer.train()\n",
        "\n",
        "print(f\"Saving fine-tuned Flan-T5 model to {ProjectConfig.FLAN_T5_OUTPUT} ...\")\n",
        "flan_t5_model.save_pretrained(ProjectConfig.FLAN_T5_OUTPUT)\n",
        "flan_t5_tokenizer.save_pretrained(ProjectConfig.FLAN_T5_OUTPUT)\n",
        "print(\"âœ… Flan-T5 model saved!\")"
      ],
      "metadata": {
        "id": "65sqUUqKZ2Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For code, test pass@1 and run a few completions\n",
        "# For text, use Trainer.evaluate() and print metrics\n",
        "# Save models to Hugging Face or GDrive"
      ],
      "metadata": {
        "id": "fN-x13k2aH_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}